:toc:
:toc-placement!:
:toclevels: 3
= CloudFiles Core

This is the documentation of the CloudFiles _core_ module which is a basic dependency of all other modules.

toc::[]

== Synopsis
The CloudFiles _core_ module defines the basic interfaces used by the CloudFiles library to interact with different types of HTTP-based file systems. Specifically, it contains the link:src/main/scala/com/github/cloudfiles/core/FileSystem.scala[FileSystem] trait defining the operations and programming model by which files and directories can be accessed across different HTTP-based protocols. Other modules typically provide a concrete implementation of these interfaces for a specific protocol.

The module also implements basic functionality related to sending and processing HTTP requests, which is needed for most HTTP-based file system implementations. The corresponding utilities in this area are specifically intended to be used by concrete `FileSystem` implementations; however, they can well be used on their own to simplify sending of HTTP requests and processing responses.

== HTTP-related utilities
Most of the protocols supported by CloudFiles are based on HTTP. This means that files or directories are manipulated by sending specific HTTP requests to a server. CloudFiles uses the https://pekko.apache.org/docs/pekko-http/current/[Apache Pekko HTTP] library for this purpose. Since there is typically a single API endpoint that accepts the protocol-specific requests, the https://pekko.apache.org/docs/pekko-http/current/client-side/host-level.html[Host-Level Client-Side API] provided by Apache Pekko HTTP is suitable.

This API is rather low-level, however. In order to enable a richer programming model, CloudFiles provides an actor implementation that allows processing HTTP requests by sending corresponding request messages to an actor instance: link:src/main/scala/com/github/cloudfiles/core/http/HttpRequestSender.scala[HttpRequestSender]. The actor internally manages a Pekko host connection pool and uses it to forward the requests to the target server. The responses are then sent to an actor instance specified in the original request message. The actor can also be used via the _ask_ pattern, and it provides a static helper function further simplifying this usage scenario. That way, the result of the request is available as a `Future`.

One advantage of this actor-based approach is that it is rather straight-forward to extend the basic functionality of sending requests by additional logic, e.g. error handling or different mechanism for authentication. HTTP libraries typically support some type of _interceptors_ for such use cases, but Apache Pekko HTTP currently does not have this feature. In CloudFiles, interceptors are modeled by special-purpose actor implementations, so-called _extensions_, that can be chained together in order to form a request/response processing pipeline: An extension actor accepts a request and can manipulate it according to its special functionality (e.g. by adding a special `Authorization` header). The (manipulated) request is then forwarded to the next actor in the chain, which is either another extension or the final `HttpRequestSender` actor which actually executes the request. The response is then passed along the same chain and can optionally be manipulated at each step.

The following sections shortly describe the single actor implementations provided by CloudFiles _core_. For a detailed description, please consult the ScalaDocs of the implementation classes.

[#_httprequestsender]
=== HttpRequestSender
As already mentioned, this is the base actor implementation for sending HTTP requests to a specific target server. It is a typed actor that handles messages of type `HttpCommand`. `HttpCommand` consists of the following sub commands:

* `SendRequest` is the most important message, since it triggers the sending of an HTTP request. The message consists of the actual request (in terms of an Apache Pekko HTTP `HttpRequest` object), an arbitrary data object which is just passed through and can be used to transport application-specific data, a reference to the actor which is going to receive the result, and the so-called _discard entity mode_ (see below). The result of the send operation is then passed to the receiver actor in form of a `Result` object, which can be either a `SuccessResult` or a `FailureResult`.
* The `Stop` command provides a means to terminate the actor instance and free all associated resources. In case of a chain with multiple extension actors, the command is propagated so that all members in the chain are gracefully terminated.
* `ForwardedResult` is mainly used internally to make chaining of extension actors possible. The command provides a way to manipulate a result or trigger additional steps before it is passed to the next actor in the chain - if any.

The underlying Apache Pekko HTTP library requires that all HTTP entities received via responses are fully read or discarded - in case they are not needed. Forgetting this could block the HTTP connection - see https://pekko.apache.org/docs/pekko-http/current/implications-of-streaming-http-entity.html[Implications of the streaming nature of Request/Response Entities]. Ensuring that all entities are consumed is not always trivial, especially if there are unexpected responses or errors. `HttpRequestSender` makes the handling of entities easier and safer by introducing a _discard entity mode_ parameter in the `SendRequest` message. Depending on the concrete value, the actor can discard the entity automatically under certain conditions. The following values are possible:

.Discard entity modes
[cols="1,3",options="header"]
|===
|Value |Description

|OnFailure
|The entity is returned as is if the response is successful (i.e. a 2xx code). If the response status indicates an error, it is automatically discarded. This is often what an application requires; therefore, this is the standard mode.

|Always
|The entity is always discarded. This mode is useful for instance for update requests for which no response data is expected or needed, e.g. DELETE or PATCH requests.

|Never
|The entity is never discarded. This mode could be appropriate if the API also returns interesting information in case of errors.
|===

There are some helper functions that simplify using the actor from non-actor clients:

* `sendRequest()` creates a `SendRequest` object from the passed in parameters and passes it to the specified actor instance. The response is then returned as a `Future`.
* `sendRequestSuccess()` works like `sendRequest()`, but it also checks whether the response status indicates a success. If not, the resulting `Future` is failed. This can make error handling easier.

=== MultiHostExtension
An instance of <<HttpRequestSender>> can be used to send HTTP requests to a single server. Some of the protocols supported by CloudFiles define multiple endpoints hosted by different servers, however. For instance, there can be one main API endpoint to manipulate files and folders, but in order to download files, a different endpoint has to be used. For such use cases, there is the link:src/main/scala/com/github/cloudfiles/core/http/MultiHostExtension.scala[MultiHostExtension] actor implementation.

An actor of this class uses a map to associate `HttpRequestSender` actors with host names. Whenever a request is to be sent, it consults the map to find out whether there is already an actor for the host referenced by the request URI. If so, it delegates the request to this actor. Otherwise, it creates a new request sender actor, stores it in the map, and executes the request on this instance. The creation of new `HttpRequestSender` actors is done by a function which can be provided when instantiating a `MultiHostExtension` actor. This makes it possible to inject customized request sender actors.

Note that this actor implementation is mainly intended for the use case described above: to support protocols that require more than a single endpoint. Nevertheless, the total number of hosts to be managed by a `MultiHostExtension` instance is expected to be low. The implementation should therefore not be used as an all-purpose actor for sending arbitrary HTTP requests; there should be more efficient ways to do this.

=== Authentication support
The HTTP-related utilities contain some extension actors that deal with different kinds of authentication. These actors intercept requests and add an `Authorization` header to them. They can be configured using concrete subclasses of the link:src/main/scala/com/github/cloudfiles/core/http/auth/AuthConfig.scala[AuthConfig] trait.

==== BasicAuthExtension
One of the easiest authentication mechanisms is Basic Auth. Here a username and a password are combined and base64-encoded; the resulting string is used as `Authorization` header. The link:src/main/scala/com/github/cloudfiles/core/http/auth/BasicAuthExtension.scala[BasicAuthExtension] actor supports this authentication mechanism. An instance is initialized with a `BasicAuthConfig` object which consists of a username and a password. Based on this information, it can generate the required header and add it to requests.

[#auth_oauth]
==== OAuthExtension
The https://oauth.net/2/[OAuth 2] protocol is another popular authentication mechanism. Here, authentication and authorization information is represented by tokens. The link:src/main/scala/com/github/cloudfiles/core/http/auth/OAuthExtension.scala[OAuthExtension] actor implements an OAuth flow which is frequently used in HTTP-based file system protocols. It assumes that an access token has already been obtained via an OAuth client whose credentials (client ID and client secret) are known. It uses this token to generate the `Authorization` header. It then monitors the outcome of the request. If the response status is 401, this is interpreted as an indication that the access token has expired. It then sends a request to the token endpoint to obtain another access token based on a refresh token.

An instance of this actor class is configured using an `OAuthConfig` object containing the following information which is required for performing a successful token refresh:

* the URI of the endpoint for obtaining an access token
* the _redirect URI_ configured for this OAuth client
* the ID of the OAuth client
* the OAuth client secret
* initial token information consisting of an access token and a refresh token

In addition, the configuration can contain a function the actor invokes when it has a done a token refresh. This allows an external party to keep track on changed tokens. A use case could be to persist the new access token, so that it can be reused for later operations; this could be useful if the token is valid for a longer period.

=== RetryAfterExtension
The purpose of this extension actor implementation is to deal with responses of the failure status 429 _Too many requests_. Practice has shown that some service providers enforce a rate limit that can be reached when executing many operations in a short time, e.g. when trying to upload a larger number of small files. In this case, the server responds with the error code 429, and the response typically contains a `Retry-After` header that defines a delay until when another request will be accepted.

`RetryAfterExtension` intercepts responses with this error code and evaluates the `Retry-After` header if it is present. If the header cannot be found or has an unexpected format, a configurable delay is used instead. The actor then waits for this time span, and afterward retries the request. Ideally, this new request is now successful; otherwise, the same steps are performed again.

=== HttpRequestSenderFactory
The extension mechanism supported by the HTTP-related utilities requires that a number of actors are created and linked together in a chain. This is in the responsibility of client applications. A frequent use case is that such a chain of extensions has to be constructed dynamically based on configuration. This is especially useful if CloudFiles is used as an abstraction over different protocols, and a concrete protocol is selected dynamically. Often, the chain of HTTP actors then depends on the selected protocol and/or the target server to interact with.

To simplify the setup of a chain of HTTP actors, at least for the standard extensions, CloudFiles offers the link:src/main/scala/com/github/cloudfiles/core/http/factory/HttpRequestSenderFactory.scala[HttpRequestSenderFactory] trait and the default implementation link:src/main/scala/com/github/cloudfiles/core/http/factory/HttpRequestSenderFactoryImpl.scala[HttpRequestSenderFactoryImpl]. The idea behind this trait is that a concrete chain is defined in terms of an link:src/main/scala/com/github/cloudfiles/core/http/factory/HttpRequestSenderConfig.scala[HttpRequestSenderConfig] object. The object contains a number of properties corresponding to the standard extensions supported by CloudFiles. Based on these properties, the factory is able to create the required actor instances, configure them correctly, and link them together. So, ideally, with a configuration object at hand, obtaining a fully initialized request sender actor is a matter of a single function call. This actor can then be used together with a CloudFiles `FileSystem` implementation to manipulate files on a server.

The factory for request actors needs a way to create new actor instances. How this is done typically depends on client code. For instance, if the client is an actor, too, new actors should probably be created via its actor context, so that they become child actors. Other types of clients may have different requirements. To abstract over potential usage scenarios, CloudFiles offers the link:src/main/scala/com/github/cloudfiles/core/http/factory/Spawner.scala[Spawner] trait. It defines a generic function for creating a new (typed) actor. There are already a number of implementations available supporting different kinds of clients, e.g. for typed actors, classic actors or making use of a classic actor system. Since the interface is quite simple, it should be straight-forward to provide a custom implementation if the available options are not sufficient.

== File systems
The basic abstraction introduced by CloudFiles is represented by the link:src/main/scala/com/github/cloudfiles/core/FileSystem.scala[FileSystem] trait. A `FileSystem` object can be used to execute typical CRUD operations on files and directories on a server that supports a specific protocol. The various submodules of CloudFiles typically provide specialized `FileSystem` implementations that support a specific protocol. Refer to the README documents of these modules for further details.

=== Type parameters
The `FileSystem` trait has a number of type parameters:

[source,scala]
----
trait FileSystem[ID, FILE, FOLDER, FOLDER_CONTENT]
----

The `ID` parameter defines the type used for the identifiers of files and folders. Many functions of the `FileSystem` trait expect a parameter of this type to select the element to be accessed. A number of file systems just use strings as identifiers, for instance the implementations for OneDrive or GoogleDrive; but more complex identifier types are common as well. The WebDav implementation is an example for this; it identifies files and folders based on Uris.

The remaining type parameters define the types used to represent the elements contained in the file system:

* the type for files
* the type for folders (which form a hierarchy in the file system)
* and a type to represent the content of a folder. This typically includes collections with the files and (sub) folders contained in this folder plus additional metadata. The `folderContent()` function returns such an object for a given folder ID.

The `FileSystem` trait does not define any constraints on these types; so a concrete implementation is free to use whatever types it finds suitable. However, to support a certain level of interoperability between different file systems, it is helpful if the types in use follow certain standards. To support this, the link:src/main/scala/com/github/cloudfiles/core/Model.scala[Model] module defines a number of traits declaring standard properties for files and folders and even a concrete data class to represent the content of a folder. The `FileSystem` implementations in CloudFiles use type parameters that extend these traits, so that basic properties can be accessed across different file systems. If applicable, the file system-specific data types provide additional properties and functionality to make special features of the underlying protocol available.

=== Monadic operations
The `FileSystem` trait follows the functional programming paradigm. Its methods do not manipulate files and folders themselves as a side effect, but return `Operation` objects that perform the desired operation(s) when invoked. An `Operation` has a `run()` method that expects a reference to an <<HttpRequestSender>> actor (such an actor is always required when dealing with file systems) and returns a `Future` with the result of the operation. So, as an example, the following code fragment obtains a file with a specific ID known beforehand:

[source,scala]
----
val httpSender = ... // create sender actor
val fileSystem = ... // create a specific FileSystem
val fileID = someID

// First obtain an operation to resolve the file.
val opResolveFile = fileSystem.resolveFile(fileID)

// Then execute the operation, result is a Future.
val futureFile = opResolveFile.run(httpSender)

// Process the file result.
futureFile onComplete {
  case Success(file) =>
    // Do something with the file
  case Failure(exception) =>
    // Handle the exception
}
----

The `Operation` type is a monad. This means that multiple instances can be combined before they are executed in a single step. For this purpose Scala's *for* comprehensions can be used. As a more complex example let's assume that the ID of the file to be resolved is not known, but only its path. Then two operations are required:

* The `FileSystem.resolvePath()` method returns an operation that can determine the ID of a file or folder specified by its path.
* The ID can then be passed to the already known `FileSystem.resolveFile()` method which returns the operation to resolve the file.

In code, this could look as follows:

[source,scala]
----
// Get a combined operation.
val opResolveFileByPath = for {
  id <- fileSystem.resolvePath(filePath)
  file <- fileSystem.resolveFile(id)
} yield file

// Then execute it.
val futureFile = opResolveFileByPath.run(httpSender)
----

NOTE: As resolving files and folders based on their path is a common use case, the `FileSystem` trait offers convenience methods that do this. So, the example is rather to demonstrate the underlying concept. However, the implementations of the existing convenience methods look exactly as shown here.

Using this approach, an arbitrary complex operation can be constructed, which is basically a description of the actions to be performed. Only when invoking the `run()` method on the resulting `Operation`, the actions are actually executed.

One advantage of this programming model is that error handling is rather straight-forward: When executing a combined operation the resulting `Future` is successful only if all primitive operations could be completed successfully. If a single operation fails, no later operations are run, and the resulting `Future` completes with the failure produced by the primitive operation.

=== Extensible file systems
In addition to defining an abstraction for accessing files and folders over multiple protocols, CloudFiles supports specific extensions on all file systems. The idea is, that a `FileSystem` implementation decorates another `FileSystem`. It uses the underlying `FileSystem` for the manipulation of files and folders according to the concrete protocol, but can implement enhanced functionality on top of this. Example use cases for such extensions could be:

* When files are uploaded their content can be compressed; analogously, downloaded files are deflated on the fly.
* Data can be stored on the server in encrypted form. Like for the compression use case, uploaded files would be encrypted dynamically, and downloaded files would again be decrypted.
* The names of files and folders could be mapped based on some scheme.

To make advanced transformations possible, as required by the use cases listed above, an extension implementation must have a controlled way to manipulate specific properties of files and folders without knowing the concrete internal representation of these objects; remember that the types used for items in the file system are defined by <<Type parameters>>. This goes beyond the standard operations offered by the `FileSystem` trait. Therefore, CloudFiles provides another trait, link:src/main/scala/com/github/cloudfiles/core/delegate/ExtensibleFileSystem.scala[ExtensibleFileSystem], which extends the basic `FileSystem` trait by the required functionality. So, only file systems implementing this `ExtensibleFileSystem` trait can be decorated by extensions. The good news is that all the standard implementations integrating concrete protocols offered by CloudFiles fall under this category. Actually, the `ExtensibleFileSystem` trait requires only two additional methods, so supporting this extension mechanism is not that hard.

To simplify the implementation of concrete extensions, CloudFiles has the link:src/main/scala/com/github/cloudfiles/core/delegate/DelegateFileSystem.scala[DelegateFileSystem] trait. It provides default implementations for all the operations defined by `FileSystem` that just forward the call to another `FileSystem` object. So, an extension implementation extending this trait just needs to override the methods affected by the specific functionality it provides and can use the default implementations for all others.

To sum up, CloudFiles' file systems can be grouped into two categories: file systems that implement the `FileSystem` API for a specific protocol, and file systems implementing extended functionality on arbitrary other file systems.
